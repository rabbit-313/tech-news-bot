# Tech News Bot - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°è¨­è¨ˆ

## ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

### å…¨ä½“æ§‹æˆå›³

```mermaid
graph TB
    %% External Data Sources
    subgraph "External Sources"
        RSS[RSS Feeds<br/>- Hacker News<br/>- TechCrunch<br/>- Zenn/Qiita]
        GitHub[GitHub API<br/>- Trending Repos<br/>- Popular Releases]
        Reddit[Reddit API<br/>- r/programming<br/>- r/webdev]
    end

    %% Scheduled Events
    CW1[CloudWatch Events<br/>Collection Cron<br/>10:00, 19:00 JST]
    CW2[CloudWatch Events<br/>Notification Cron<br/>10:30, 19:30 JST]

    %% Lambda Functions
    subgraph "AWS Lambda"
        Collector[Collector Function<br/>Memory: 1024MB<br/>Timeout: 5min]
        Notifier[Notifier Function<br/>Memory: 256MB<br/>Timeout: 1min]
    end

    %% Processing Components
    subgraph "Processing Pipeline"
        Filter[Content Filter<br/>- Spam Detection<br/>- Language Filter<br/>- Quality Check]
        Dedup[Deduplication<br/>- Content Hash<br/>- URL Similarity<br/>- Title Similarity]
        AI[AI Processing<br/>- OpenAI GPT-4<br/>- Summarization<br/>- Scoring]
    end

    %% Data Storage
    subgraph "Data Layer"
        DDB[(DynamoDB<br/>Articles Table<br/>TTL: 30 days)]
        SSM[SSM Parameter Store<br/>- API Keys<br/>- Webhook URLs<br/>- Config]
    end

    %% Output
    Slack[Slack Channel<br/>Daily Summary<br/>Top 10 Articles]

    %% Monitoring
    subgraph "Monitoring"
        CWLogs[CloudWatch Logs<br/>Structured Logging]
        CWMetrics[CloudWatch Metrics<br/>Custom Metrics]
        CWAlarms[CloudWatch Alarms<br/>Error Detection]
    end

    %% Flow Connections
    CW1 --> Collector
    RSS --> Collector
    GitHub --> Collector
    Reddit --> Collector
    
    Collector --> Filter
    Filter --> Dedup
    Dedup --> AI
    AI --> DDB
    
    CW2 --> Notifier
    DDB --> Notifier
    Notifier --> Slack
    
    SSM --> Collector
    SSM --> Notifier
    
    Collector --> CWLogs
    Notifier --> CWLogs
    Collector --> CWMetrics
    Notifier --> CWMetrics
    CWMetrics --> CWAlarms
```

## ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè©³ç´°è¨­è¨ˆ

### 1. ãƒ‡ãƒ¼ã‚¿åé›†å±¤ (Collection Layer)

#### 1.1 Collector Lambda Function

**å®Ÿè¡Œä»•æ§˜:**
- Runtime: Python 3.11
- Memory: 1024MB
- Timeout: 5åˆ†
- å®Ÿè¡Œé »åº¦: 1æ—¥2å› (10:00, 19:00 JST)

**å‡¦ç†ãƒ•ãƒ­ãƒ¼:**
```python
def collector_handler(event, context):
    """
    è¨˜äº‹åé›†ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    """
    # 1. å„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ä¸¦è¡Œåé›†
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [
            executor.submit(RSSCollector().collect),
            executor.submit(GitHubCollector().collect),
            executor.submit(RedditCollector().collect)
        ]
        
        all_articles = []
        for future in as_completed(futures):
            articles = future.result()
            all_articles.extend(articles)
    
    # 2. ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    filtered_articles = ContentFilter().filter(all_articles)
    unique_articles = Deduplicator().remove_duplicates(filtered_articles)
    processed_articles = AIProcessor().process(unique_articles)
    
    # 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
    ArticleStore().bulk_save(processed_articles)
    
    return {
        "statusCode": 200,
        "collected": len(all_articles),
        "processed": len(processed_articles)
    }
```

#### 1.2 ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è©³ç´°

**RSS Collector:**
```python
class RSSCollector(BaseCollector):
    FEEDS = [
        {
            "url": "https://hnrss.org/frontpage",
            "source": "hackernews",
            "weight": 0.9,
            "language": "en"
        },
        {
            "url": "https://japan.techcrunch.com/feed/",
            "source": "techcrunch",
            "weight": 0.8,
            "language": "ja"
        },
        {
            "url": "https://zenn.dev/feed",
            "source": "zenn",
            "weight": 0.7,
            "language": "ja"
        }
    ]
    
    def collect(self) -> List[Article]:
        articles = []
        for feed_config in self.FEEDS:
            try:
                feed = feedparser.parse(feed_config["url"])
                for entry in feed.entries:
                    article = self._parse_entry(entry, feed_config)
                    if self._is_recent(article) and self._is_tech_related(article):
                        articles.append(article)
            except Exception as e:
                logger.error(f"RSS collection failed: {feed_config['url']}", error=str(e))
        
        return articles
```

**GitHub Collector:**
```python
class GitHubCollector(BaseCollector):
    def collect(self) -> List[Article]:
        articles = []
        
        # GitHub Trending repositories
        trending_repos = self._get_trending_repositories()
        for repo in trending_repos:
            article = Article(
                title=f"ğŸ“ˆ Trending: {repo['full_name']}",
                url=repo['html_url'],
                summary=repo['description'],
                published_at=datetime.fromisoformat(repo['created_at']),
                source="github_trending",
                tags=repo['topics'] + ["github", "trending"],
                score=self._calculate_github_score(repo)
            )
            articles.append(article)
        
        # Popular releases
        recent_releases = self._get_popular_releases()
        for release in recent_releases:
            article = self._create_release_article(release)
            articles.append(article)
        
        return articles
    
    def _calculate_github_score(self, repo: dict) -> float:
        """GitHub ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        stars = repo.get('stargazers_count', 0)
        forks = repo.get('forks_count', 0)
        language = repo.get('language', '')
        
        # åŸºæœ¬ã‚¹ã‚³ã‚¢ (staræ•°ãƒ™ãƒ¼ã‚¹)
        base_score = min(stars / 10000, 0.8)
        
        # è¨€èªãƒœãƒ¼ãƒŠã‚¹
        tech_languages = ['Python', 'JavaScript', 'TypeScript', 'Go', 'Rust']
        language_bonus = 0.1 if language in tech_languages else 0
        
        # ãƒ•ã‚©ãƒ¼ã‚¯ç‡ãƒœãƒ¼ãƒŠã‚¹
        fork_ratio = forks / max(stars, 1)
        fork_bonus = min(fork_ratio * 0.1, 0.1)
        
        return min(base_score + language_bonus + fork_bonus, 1.0)
```

**Reddit Collector:**
```python
class RedditCollector(BaseCollector):
    SUBREDDITS = [
        {"name": "programming", "weight": 0.8},
        {"name": "webdev", "weight": 0.7},
        {"name": "MachineLearning", "weight": 0.9},
        {"name": "technology", "weight": 0.6}
    ]
    
    def collect(self) -> List[Article]:
        reddit = praw.Reddit(
            client_id=self.config.reddit_client_id,
            client_secret=self.config.reddit_client_secret,
            user_agent="TechNewsBot/1.0"
        )
        
        articles = []
        for sub_config in self.SUBREDDITS:
            subreddit = reddit.subreddit(sub_config["name"])
            
            # äººæ°—æŠ•ç¨¿ã‚’å–å¾— (24æ™‚é–“ä»¥å†…)
            for submission in subreddit.hot(limit=20):
                if self._is_recent_submission(submission):
                    article = self._create_reddit_article(submission, sub_config)
                    articles.append(article)
        
        return articles
    
    def _calculate_reddit_score(self, submission, weight: float) -> float:
        """Reddit æŠ•ç¨¿ã®ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        upvotes = submission.score
        comments = submission.num_comments
        ratio = submission.upvote_ratio
        
        # åŸºæœ¬ã‚¹ã‚³ã‚¢
        base_score = min(upvotes / 1000, 0.7)
        
        # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãƒœãƒ¼ãƒŠã‚¹
        comment_bonus = min(comments / 100 * 0.1, 0.2)
        
        # ã‚¢ãƒƒãƒ—ãƒ´ã‚©ãƒ¼ãƒˆç‡ãƒœãƒ¼ãƒŠã‚¹
        ratio_bonus = (ratio - 0.5) * 0.2
        
        return min((base_score + comment_bonus + ratio_bonus) * weight, 1.0)
```

### 2. ãƒ‡ãƒ¼ã‚¿å‡¦ç†å±¤ (Processing Layer)

#### 2.1 Content Filter

```python
class ContentFilter:
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    
    TECH_KEYWORDS = [
        'python', 'javascript', 'react', 'ai', 'machine learning',
        'kubernetes', 'docker', 'aws', 'api', 'framework',
        'open source', 'github', 'programming', 'development'
    ]
    
    SPAM_PATTERNS = [
        r'win \$\d+',
        r'click here',
        r'limited time offer',
        r'buy now'
    ]
    
    def filter(self, articles: List[Article]) -> List[Article]:
        """è¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        filtered = []
        
        for article in articles:
            if self._is_tech_related(article) and not self._is_spam(article):
                filtered.append(article)
        
        return filtered
    
    def _is_tech_related(self, article: Article) -> bool:
        """ãƒ†ãƒƒã‚¯é–¢é€£è¨˜äº‹ã‹ã©ã†ã‹åˆ¤å®š"""
        content = f"{article.title} {article.summary}".lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        keyword_score = sum(1 for keyword in self.TECH_KEYWORDS if keyword in content)
        
        # ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š
        tech_tags = {'programming', 'tech', 'ai', 'ml', 'web', 'mobile'}
        tag_score = len(set(article.tags) & tech_tags)
        
        return keyword_score >= 1 or tag_score >= 1
    
    def _is_spam(self, article: Article) -> bool:
        """ã‚¹ãƒ‘ãƒ è¨˜äº‹ã‹ã©ã†ã‹åˆ¤å®š"""
        content = f"{article.title} {article.summary}".lower()
        
        for pattern in self.SPAM_PATTERNS:
            if re.search(pattern, content, re.IGNORECASE):
                return True
        
        return False
```

#### 2.2 Deduplicator

```python
class Deduplicator:
    """é‡è¤‡é™¤å»å‡¦ç†"""
    
    def remove_duplicates(self, articles: List[Article]) -> List[Article]:
        """è¨˜äº‹ã®é‡è¤‡ã‚’é™¤å»"""
        seen_hashes = set()
        unique_articles = []
        
        # æ—¢å­˜è¨˜äº‹ã®ãƒãƒƒã‚·ãƒ¥ã‚’å–å¾—
        existing_hashes = self._get_existing_hashes()
        seen_hashes.update(existing_hashes)
        
        for article in articles:
            content_hash = self._generate_content_hash(article)
            article.content_hash = content_hash
            
            if content_hash not in seen_hashes:
                # URLé¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯
                if not self._is_similar_url_exists(article, unique_articles):
                    unique_articles.append(article)
                    seen_hashes.add(content_hash)
        
        return unique_articles
    
    def _generate_content_hash(self, article: Article) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ"""
        content = f"{article.title}{article.url}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]
    
    def _is_similar_url_exists(self, article: Article, existing: List[Article]) -> bool:
        """é¡ä¼¼URLã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        from difflib import SequenceMatcher
        
        for existing_article in existing:
            similarity = SequenceMatcher(None, article.url, existing_article.url).ratio()
            if similarity > 0.8:  # 80%ä»¥ä¸Šã®é¡ä¼¼åº¦
                return True
        
        return False
```

#### 2.3 AI Processor

```python
class AIProcessor:
    """AI ã‚’ä½¿ç”¨ã—ãŸè¨˜äº‹å‡¦ç†"""
    
    def __init__(self):
        self.openai_client = OpenAI(api_key=config.openai_api_key)
    
    def process(self, articles: List[Article]) -> List[Article]:
        """è¨˜äº‹ã‚’AIå‡¦ç†"""
        processed_articles = []
        
        for article in articles:
            try:
                # è¦ç´„ç”Ÿæˆï¼ˆè¦ç´„ãŒãªã„å ´åˆã®ã¿ï¼‰
                if not article.summary or len(article.summary) < 50:
                    article.summary = self._generate_summary(article)
                
                # ã‚¹ã‚³ã‚¢è¨ˆç®—
                article.score = self._calculate_ai_score(article)
                
                # ã‚¿ã‚°æ‹¡å……
                article.tags.extend(self._extract_ai_tags(article))
                
                processed_articles.append(article)
                
            except Exception as e:
                logger.error(f"AI processing failed for article: {article.title}", error=str(e))
                # AIå‡¦ç†å¤±æ•—æ™‚ã¯ãã®ã¾ã¾è¿½åŠ 
                processed_articles.append(article)
        
        return processed_articles
    
    def _generate_summary(self, article: Article) -> str:
        """è¨˜äº‹è¦ç´„ã‚’ç”Ÿæˆ"""
        prompt = f"""
        ä»¥ä¸‹ã®æŠ€è¡“è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã‹ã‚‰ã€æ—¥æœ¬èªã§ç°¡æ½”ãªè¦ç´„ï¼ˆ50-100æ–‡å­—ï¼‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        æŠ€è¡“çš„ãªå†…å®¹ã‚’é‡è¦–ã—ã€èª­è€…ã«ã¨ã£ã¦æœ‰ç›Šãªæƒ…å ±ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
        
        ã‚¿ã‚¤ãƒˆãƒ«: {article.title}
        URL: {article.url}
        """
        
        response = self.openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150,
            temperature=0.3
        )
        
        return response.choices[0].message.content.strip()
    
    def _calculate_ai_score(self, article: Article) -> float:
        """AI ã«ã‚ˆã‚‹è¨˜äº‹ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        prompt = f"""
        ä»¥ä¸‹ã®æŠ€è¡“è¨˜äº‹ã®é‡è¦åº¦ã‚’0.0ã€œ1.0ã®ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
        è©•ä¾¡åŸºæº–:
        - æŠ€è¡“çš„æ–°è¦æ€§ãƒ»é©æ–°æ€§
        - å®Ÿç”¨æ€§ãƒ»æœ‰ç”¨æ€§
        - å½±éŸ¿åŠ›ãƒ»è©±é¡Œæ€§
        - æƒ…å ±ã®ä¿¡é ¼æ€§
        
        ã‚¿ã‚¤ãƒˆãƒ«: {article.title}
        è¦ç´„: {article.summary}
        ã‚½ãƒ¼ã‚¹: {article.source}
        
        æ•°å€¤ã®ã¿è¿”ç­”ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 0.75ï¼‰
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=10,
                temperature=0.1
            )
            
            score_text = response.choices[0].message.content.strip()
            return float(score_text)
            
        except Exception:
            # AI ã‚¹ã‚³ã‚¢è¨ˆç®—å¤±æ•—æ™‚ã¯åŸºæœ¬ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨
            return self._calculate_basic_score(article)
    
    def _calculate_basic_score(self, article: Article) -> float:
        """åŸºæœ¬çš„ãªã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆAIå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        score = 0.5  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        
        # ã‚½ãƒ¼ã‚¹åˆ¥é‡ã¿ä»˜ã‘
        source_weights = {
            'hackernews': 0.9,
            'github_trending': 0.8,
            'techcrunch': 0.7,
            'zenn': 0.6,
            'reddit': 0.5
        }
        
        source_weight = source_weights.get(article.source, 0.5)
        score *= source_weight
        
        # ã‚¿ã‚¤ãƒˆãƒ«é•·ãƒœãƒ¼ãƒŠã‚¹ï¼ˆé©åº¦ãªé•·ã•ï¼‰
        title_len = len(article.title)
        if 20 <= title_len <= 100:
            score += 0.1
        
        return min(score, 1.0)
```

### 3. ãƒ‡ãƒ¼ã‚¿ä¿å­˜å±¤ (Storage Layer)

#### 3.1 DynamoDB ãƒ†ãƒ¼ãƒ–ãƒ«è¨­è¨ˆ

```yaml
Table: tech-news-articles
Properties:
  BillingMode: PAY_PER_REQUEST
  
Attributes:
  date: String (YYYY-MM-DD) # Partition Key
  article_id: String (UUID) # Sort Key
  title: String
  url: String
  summary: String
  published_at: Number (Unix timestamp)
  source: String
  tags: List
  score: Number (0.0-1.0)
  content_hash: String
  author: String (Optional)
  ttl: Number (è‡ªå‹•å‰Šé™¤ç”¨)

Global Secondary Indexes:
  1. source-published_at-index
     - PK: source
     - SK: published_at
     - Purpose: ã‚½ãƒ¼ã‚¹åˆ¥ã®è¨˜äº‹å–å¾—
  
  2. content-hash-index
     - PK: content_hash
     - Purpose: é‡è¤‡æ¤œå‡º
  
  3. score-index
     - PK: date
     - SK: score
     - Purpose: é«˜ã‚¹ã‚³ã‚¢è¨˜äº‹ã®å–å¾—
```

#### 3.2 Article Store Implementation

```python
class ArticleStore:
    """DynamoDB ã‚’ä½¿ç”¨ã—ãŸè¨˜äº‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸"""
    
    def __init__(self):
        self.dynamodb = boto3.resource('dynamodb')
        self.table = self.dynamodb.Table(config.dynamodb_table)
    
    def bulk_save(self, articles: List[Article]) -> int:
        """è¨˜äº‹ã‚’ä¸€æ‹¬ä¿å­˜"""
        saved_count = 0
        
        with self.table.batch_writer() as batch:
            for article in articles:
                try:
                    item = self._article_to_dynamodb_item(article)
                    batch.put_item(Item=item)
                    saved_count += 1
                except Exception as e:
                    logger.error(f"Failed to save article: {article.title}", error=str(e))
        
        return saved_count
    
    def get_articles_by_date(self, date: datetime.date, min_score: float = 0.0) -> List[Article]:
        """æ—¥ä»˜æŒ‡å®šã§è¨˜äº‹ã‚’å–å¾—"""
        date_str = date.strftime('%Y-%m-%d')
        
        response = self.table.query(
            KeyConditionExpression=Key('date').eq(date_str),
            FilterExpression=Attr('score').gte(min_score),
            ScanIndexForward=False  # æ–°ã—ã„é †
        )
        
        articles = []
        for item in response['Items']:
            article = self._dynamodb_item_to_article(item)
            articles.append(article)
        
        return articles
    
    def get_existing_hashes(self, days: int = 7) -> Set[str]:
        """éå»Næ—¥é–“ã®è¨˜äº‹ãƒãƒƒã‚·ãƒ¥ã‚’å–å¾—"""
        hashes = set()
        
        for i in range(days):
            date = datetime.date.today() - datetime.timedelta(days=i)
            date_str = date.strftime('%Y-%m-%d')
            
            response = self.table.query(
                KeyConditionExpression=Key('date').eq(date_str),
                ProjectionExpression='content_hash'
            )
            
            for item in response['Items']:
                hashes.add(item['content_hash'])
        
        return hashes
    
    def _article_to_dynamodb_item(self, article: Article) -> dict:
        """Article ã‚’ DynamoDB ã‚¢ã‚¤ãƒ†ãƒ ã«å¤‰æ›"""
        date_str = article.published_at.strftime('%Y-%m-%d')
        ttl = int((datetime.now() + timedelta(days=30)).timestamp())
        
        return {
            'date': date_str,
            'article_id': str(uuid.uuid4()),
            'title': article.title,
            'url': article.url,
            'summary': article.summary,
            'published_at': int(article.published_at.timestamp()),
            'source': article.source,
            'tags': article.tags,
            'score': Decimal(str(article.score)),
            'content_hash': article.content_hash,
            'author': article.author,
            'ttl': ttl
        }
```

### 4. é€šçŸ¥å±¤ (Notification Layer)

#### 4.1 Notifier Lambda Function

```python
def notifier_handler(event, context):
    """Slacké€šçŸ¥ã®ãƒ¡ã‚¤ãƒ³ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    # æœ¬æ—¥ã®é«˜ã‚¹ã‚³ã‚¢è¨˜äº‹ã‚’å–å¾—
    article_store = ArticleStore()
    today_articles = article_store.get_articles_by_date(
        datetime.date.today(),
        min_score=0.6  # ã‚¹ã‚³ã‚¢0.6ä»¥ä¸Š
    )
    
    if not today_articles:
        logger.info("No articles to notify")
        return {"statusCode": 200, "message": "No articles"}
    
    # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
    sorted_articles = sorted(today_articles, key=lambda x: x.score, reverse=True)
    top_articles = sorted_articles[:10]  # ä¸Šä½10è¨˜äº‹
    
    # æ™‚é–“å¸¯ã«ã‚ˆã‚‹åˆ†é¡
    current_hour = datetime.now().hour
    if current_hour < 12:
        notification_type = "morning"
        title = "ğŸŒ… ä»Šæ—¥ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæœåˆŠï¼‰"
    else:
        notification_type = "evening"
        title = "ğŸŒ† ä»Šæ—¥ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆå¤•åˆŠï¼‰"
    
    # Slacké€šçŸ¥
    slack_notifier = SlackNotifier()
    success = slack_notifier.send_daily_summary(top_articles, title, notification_type)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡
    cloudwatch = boto3.client('cloudwatch')
    cloudwatch.put_metric_data(
        Namespace='TechNewsBot',
        MetricData=[
            {
                'MetricName': 'NotificationsSent',
                'Value': 1 if success else 0,
                'Unit': 'Count',
                'Dimensions': [
                    {'Name': 'Type', 'Value': notification_type}
                ]
            },
            {
                'MetricName': 'ArticlesNotified',
                'Value': len(top_articles),
                'Unit': 'Count'
            }
        ]
    )
    
    return {
        "statusCode": 200 if success else 500,
        "articles_sent": len(top_articles),
        "notification_type": notification_type
    }
```

#### 4.2 Slack Notifier Implementation

```python
class SlackNotifier:
    """Slack é€šçŸ¥å‡¦ç†"""
    
    def __init__(self):
        self.webhook_url = config.slack_webhook_url
    
    def send_daily_summary(self, articles: List[Article], title: str, notification_type: str) -> bool:
        """æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’Slackã«é€ä¿¡"""
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
        blocks = [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": title
                }
            },
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": f"ğŸ“Š {len(articles)}ä»¶ã®è¨˜äº‹ã‚’ãŠå±Šã‘ã—ã¾ã™"
                    }
                ]
            },
            {"type": "divider"}
        ]
        
        # è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯
        for i, article in enumerate(articles, 1):
            blocks.extend(self._create_article_blocks(article, i))
            
            # 5è¨˜äº‹ã”ã¨ã«åŒºåˆ‡ã‚Šç·š
            if i % 5 == 0 and i < len(articles):
                blocks.append({"type": "divider"})
        
        # ãƒ•ãƒƒã‚¿ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
        blocks.extend([
            {"type": "divider"},
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": f"ğŸ¤– Tech News Bot | {datetime.now().strftime('%Y-%m-%d %H:%M')}"
                    }
                ]
            }
        ])
        
        payload = {
            "blocks": blocks,
            "username": "Tech News Bot",
            "icon_emoji": ":newspaper:"
        }
        
        try:
            response = requests.post(
                self.webhook_url,
                json=payload,
                timeout=10
            )
            
            if response.status_code == 200:
                logger.info(f"Successfully sent {len(articles)} articles to Slack")
                return True
            else:
                logger.error(f"Slack notification failed: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"Slack notification error: {str(e)}")
            return False
    
    def _create_article_blocks(self, article: Article, index: int) -> List[dict]:
        """å€‹åˆ¥è¨˜äº‹ã®Slackãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œæˆ"""
        
        # ã‚¹ã‚³ã‚¢ã«å¿œã˜ãŸçµµæ–‡å­—
        if article.score >= 0.9:
            score_emoji = "ğŸ”¥"
        elif article.score >= 0.8:
            score_emoji = "â­"
        elif article.score >= 0.7:
            score_emoji = "ğŸ‘"
        else:
            score_emoji = "ğŸ“„"
        
        # ã‚½ãƒ¼ã‚¹çµµæ–‡å­—ãƒãƒƒãƒ”ãƒ³ã‚°
        source_emojis = {
            'hackernews': 'ğŸŸ ',
            'github_trending': 'ğŸ™',
            'techcrunch': 'ğŸŸ¢',
            'zenn': 'ğŸ‡¯ğŸ‡µ',
            'reddit': 'ğŸ”¶'
        }
        source_emoji = source_emojis.get(article.source, 'ğŸ“°')
        
        # ã‚¿ã‚°ã®è¡¨ç¤ºï¼ˆæœ€å¤§3å€‹ï¼‰
        tag_text = ""
        if article.tags:
            displayed_tags = article.tags[:3]
            tag_text = " | " + " ".join([f"`{tag}`" for tag in displayed_tags])
        
        return [
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"{score_emoji} *{index}. {article.title}*\n{article.summary}"
                },
                "accessory": {
                    "type": "button",
                    "text": {
                        "type": "plain_text",
                        "text": "Read More",
                        "emoji": True
                    },
                    "url": article.url,
                    "action_id": f"read_article_{index}"
                }
            },
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": f"{source_emoji} {article.source} | Score: {article.score:.2f}{tag_text}"
                    }
                ]
            }
        ]
```

### 5. ç›£è¦–ãƒ»é‹ç”¨å±¤ (Monitoring Layer)

#### 5.1 CloudWatch ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
class MetricsCollector:
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
    
    def __init__(self):
        self.cloudwatch = boto3.client('cloudwatch')
    
    def send_collection_metrics(self, source: str, articles_count: int, processing_time: float, errors: int = 0):
        """åé›†ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é€ä¿¡"""
        self.cloudwatch.put_metric_data(
            Namespace='TechNewsBot/Collection',
            MetricData=[
                {
                    'MetricName': 'ArticlesCollected',
                    'Dimensions': [{'Name': 'Source', 'Value': source}],
                    'Value': articles_count,
                    'Unit': 'Count',
                    'Timestamp': datetime.utcnow()
                },
                {
                    'MetricName': 'ProcessingTime',
                    'Dimensions': [{'Name': 'Source', 'Value': source}],
                    'Value': processing_time,
                    'Unit': 'Seconds'
                },
                {
                    'MetricName': 'CollectionErrors',
                    'Dimensions': [{'Name': 'Source', 'Value': source}],
                    'Value': errors,
                    'Unit': 'Count'
                }
            ]
        )
    
    def send_processing_metrics(self, duplicates_removed: int, ai_failures: int, total_processed: int):
        """å‡¦ç†ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é€ä¿¡"""
        self.cloudwatch.put_metric_data(
            Namespace='TechNewsBot/Processing',
            MetricData=[
                {
                    'MetricName': 'DuplicatesRemoved',
                    'Value': duplicates_removed,
                    'Unit': 'Count'
                },
                {
                    'MetricName': 'AIProcessingFailures',
                    'Value': ai_failures,
                    'Unit': 'Count'
                },
                {
                    'MetricName': 'ArticlesProcessed',
                    'Value': total_processed,
                    'Unit': 'Count'
                }
            ]
        )
```

#### 5.2 CloudWatch ã‚¢ãƒ©ãƒ¼ãƒ è¨­å®š

```yaml
# CloudFormation Template for Alarms
Resources:
  # åé›†ã‚¨ãƒ©ãƒ¼ã‚¢ãƒ©ãƒ¼ãƒ 
  CollectionErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: TechNewsBot-CollectionErrors
      AlarmDescription: "High error rate in article collection"
      MetricName: Errors
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref CollectorFunction
      AlarmActions:
        - !Ref SNSAlertTopic
  
  # è¨˜äº‹åé›†æ•°ã‚¢ãƒ©ãƒ¼ãƒ 
  NoArticlesAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: TechNewsBot-NoArticlesCollected
      AlarmDescription: "No articles collected in recent execution"
      MetricName: ArticlesCollected
      Namespace: TechNewsBot/Collection
      Statistic: Sum
      Period: 3600
      EvaluationPeriods: 1
      Threshold: 5
      ComparisonOperator: LessThanThreshold
      TreatMissingData: breaching
      AlarmActions:
        - !Ref SNSAlertTopic
  
  # Lambdaå®Ÿè¡Œæ™‚é–“ã‚¢ãƒ©ãƒ¼ãƒ 
  LongExecutionAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: TechNewsBot-LongExecution
      AlarmDescription: "Lambda execution time too long"
      MetricName: Duration
      Namespace: AWS/Lambda
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: 240000  # 4åˆ†
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref CollectorFunction
```

#### 5.3 æ§‹é€ åŒ–ãƒ­ã‚°å®Ÿè£…

```python
import structlog
import json
from datetime import datetime

# ãƒ­ã‚°è¨­å®š
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.dev.ConsoleRenderer() if DEBUG else structlog.processors.JSONRenderer()
    ],
    wrapper_class=structlog.stdlib.LoggerFactory(),
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# ä½¿ç”¨ä¾‹
class BaseCollector:
    def __init__(self):
        self.logger = structlog.get_logger(self.__class__.__name__)
    
    def collect(self) -> List[Article]:
        start_time = time.time()
        
        try:
            articles = self._do_collect()
            processing_time = time.time() - start_time
            
            self.logger.info(
                "collection_completed",
                source=self.source_name,
                articles_collected=len(articles),
                processing_time_seconds=processing_time,
                success=True
            )
            
            return articles
            
        except Exception as e:
            processing_time = time.time() - start_time
            
            self.logger.error(
                "collection_failed",
                source=self.source_name,
                error=str(e),
                error_type=type(e).__name__,
                processing_time_seconds=processing_time,
                success=False
            )
            
            raise
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. ä¸¦è¡Œå‡¦ç†ã®å®Ÿè£…

```python
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor, as_completed

class AsyncCollector:
    """éåŒæœŸãƒ‡ãƒ¼ã‚¿åé›†"""
    
    async def collect_all_sources(self) -> List[Article]:
        """å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰ä¸¦è¡Œåé›†"""
        
        tasks = [
            self.collect_rss_async(),
            self.collect_github_async(),
            self.collect_reddit_async()
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_articles = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Collection failed: {result}")
            else:
                all_articles.extend(result)
        
        return all_articles
    
    async def collect_rss_async(self) -> List[Article]:
        """RSSéåŒæœŸåé›†"""
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            for feed_config in RSS_FEEDS:
                task = self.fetch_rss_feed(session, feed_config)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            articles = []
            for result in results:
                if not isinstance(result, Exception):
                    articles.extend(result)
            
            return articles
```

### 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

```python
import redis
from functools import wraps

class CacheManager:
    """Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    
    def __init__(self):
        self.redis_client = redis.Redis(
            host=config.redis_host,
            port=config.redis_port,
            decode_responses=True
        )
    
    def cache_articles(self, key: str, articles: List[Article], ttl: int = 3600):
        """è¨˜äº‹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        serialized = json.dumps([article.__dict__ for article in articles], default=str)
        self.redis_client.setex(key, ttl, serialized)
    
    def get_cached_articles(self, key: str) -> Optional[List[Article]]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
        cached = self.redis_client.get(key)
        if cached:
            data = json.loads(cached)
            return [Article(**item) for item in data]
        return None

def cache_result(cache_key: str, ttl: int = 3600):
    """ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ï¼šçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            cache_manager = CacheManager()
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cached_result = cache_manager.get_cached_articles(cache_key)
            if cached_result:
                return cached_result
            
            # å®Ÿè¡Œ & ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            result = func(*args, **kwargs)
            cache_manager.cache_articles(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

### 3. ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–

```python
class BatchProcessor:
    """ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–"""
    
    def __init__(self, batch_size: int = 25):
        self.batch_size = batch_size
    
    def process_articles_in_batches(self, articles: List[Article]) -> List[Article]:
        """è¨˜äº‹ã‚’ãƒãƒƒãƒå‡¦ç†"""
        processed_articles = []
        
        for i in range(0, len(articles), self.batch_size):
            batch = articles[i:i + self.batch_size]
            batch_result = self._process_batch(batch)
            processed_articles.extend(batch_result)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
            time.sleep(0.1)
        
        return processed_articles
    
    def _process_batch(self, batch: List[Article]) -> List[Article]:
        """ãƒãƒƒãƒå˜ä½ã®å‡¦ç†"""
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            
            for article in batch:
                future = executor.submit(self._process_single_article, article)
                futures.append(future)
            
            results = []
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=30)
                    results.append(result)
                except Exception as e:
                    logger.error(f"Batch processing failed: {e}")
            
            return results
```

ã“ã®è©³ç´°è¨­è¨ˆã«ã‚ˆã‚Šã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§ä¿å®ˆæ€§ã®é«˜ã„ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ç‹¬ç«‹æ€§ã‚’ä¿ã¡ãªãŒã‚‰ã€åŠ¹ç‡çš„ã«é€£æºã™ã‚‹è¨­è¨ˆã¨ãªã£ã¦ã„ã¾ã™ã€‚