# Tech News Bot - „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£Ë©≥Á¥∞Ë®≠Ë®à

## „Ç∑„Çπ„ÉÜ„É†„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£Ê¶ÇË¶Å

### ÂÖ®‰ΩìÊßãÊàêÂõ≥

```mermaid
graph TB
    %% External Data Sources
    subgraph "External Sources"
        RSS[RSS Feeds<br/>- Hacker News<br/>- TechCrunch<br/>- Zenn/Qiita]
        GitHub[GitHub API<br/>- Trending Repos<br/>- Popular Releases]
        Reddit[Reddit API<br/>- r/programming<br/>- r/webdev]
    end

    %% Scheduled Events
    CW1[CloudWatch Events<br/>Collection Cron<br/>10:00, 19:00 JST]
    CW2[CloudWatch Events<br/>Notification Cron<br/>10:30, 19:30 JST]

    %% Lambda Functions
    subgraph "AWS Lambda"
        Collector[Collector Function<br/>Memory: 1024MB<br/>Timeout: 5min]
        Notifier[Notifier Function<br/>Memory: 256MB<br/>Timeout: 1min]
    end

    %% Processing Components
    subgraph "Processing Pipeline"
        Filter[Content Filter<br/>- Spam Detection<br/>- Language Filter<br/>- Quality Check]
        Dedup[Deduplication<br/>- Content Hash<br/>- URL Similarity<br/>- Title Similarity]
        AI[AI Processing<br/>- OpenAI GPT-4<br/>- Summarization<br/>- Scoring]
    end

    %% Data Storage
    subgraph "Data Layer"
        DDB[(DynamoDB<br/>Articles Table<br/>TTL: 30 days)]
        SSM[SSM Parameter Store<br/>- API Keys<br/>- Webhook URLs<br/>- Config]
    end

    %% Output
    Slack[Slack Channel<br/>Daily Summary<br/>Top 10 Articles]

    %% Monitoring
    subgraph "Monitoring"
        CWLogs[CloudWatch Logs<br/>Structured Logging]
        CWMetrics[CloudWatch Metrics<br/>Custom Metrics]
        CWAlarms[CloudWatch Alarms<br/>Error Detection]
    end

    %% Flow Connections
    CW1 --> Collector
    RSS --> Collector
    GitHub --> Collector
    Reddit --> Collector
    
    Collector --> Filter
    Filter --> Dedup
    Dedup --> AI
    AI --> DDB
    
    CW2 --> Notifier
    DDB --> Notifier
    Notifier --> Slack
    
    SSM --> Collector
    SSM --> Notifier
    
    Collector --> CWLogs
    Notifier --> CWLogs
    Collector --> CWMetrics
    Notifier --> CWMetrics
    CWMetrics --> CWAlarms
```

## „Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàË©≥Á¥∞Ë®≠Ë®à

### 1. „Éá„Éº„ÇøÂèéÈõÜÂ±§ (Collection Layer)

#### 1.1 Collector Lambda Function

**ÂÆüË°å‰ªïÊßò:**
- Runtime: Python 3.11
- Memory: 1024MB
- Timeout: 5ÂàÜ
- ÂÆüË°åÈ†ªÂ∫¶: 1Êó•2Âõû (10:00, 19:00 JST)

**Âá¶ÁêÜ„Éï„É≠„Éº:**
```python
def collector_handler(event, context):
    """
    Ë®ò‰∫ãÂèéÈõÜ„ÅÆ„É°„Ç§„É≥„Ç®„É≥„Éà„É™„Éº„Éù„Ç§„É≥„Éà
    """
    # 1. ÂêÑ„Éá„Éº„Çø„ÇΩ„Éº„Çπ„Åã„Çâ‰∏¶Ë°åÂèéÈõÜ
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [
            executor.submit(RSSCollector().collect),
            executor.submit(GitHubCollector().collect),
            executor.submit(RedditCollector().collect)
        ]
        
        all_articles = []
        for future in as_completed(futures):
            articles = future.result()
            all_articles.extend(articles)
    
    # 2. „Éá„Éº„ÇøÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥
    filtered_articles = ContentFilter().filter(all_articles)
    unique_articles = Deduplicator().remove_duplicates(filtered_articles)
    processed_articles = AIProcessor().process(unique_articles)
    
    # 3. „Éá„Éº„Çø„Éô„Éº„Çπ‰øùÂ≠ò
    ArticleStore().bulk_save(processed_articles)
    
    return {
        "statusCode": 200,
        "collected": len(all_articles),
        "processed": len(processed_articles)
    }
```

#### 1.2 „Éá„Éº„Çø„ÇΩ„Éº„ÇπË©≥Á¥∞

**RSS Collector:**
```python
class RSSCollector(BaseCollector):
    FEEDS = [
        {
            "url": "https://hnrss.org/frontpage",
            "source": "hackernews",
            "weight": 0.9,
            "language": "en"
        },
        {
            "url": "https://japan.techcrunch.com/feed/",
            "source": "techcrunch",
            "weight": 0.8,
            "language": "ja"
        },
        {
            "url": "https://zenn.dev/feed",
            "source": "zenn",
            "weight": 0.7,
            "language": "ja"
        }
    ]
    
    def collect(self) -> List[Article]:
        articles = []
        for feed_config in self.FEEDS:
            try:
                feed = feedparser.parse(feed_config["url"])
                for entry in feed.entries:
                    article = self._parse_entry(entry, feed_config)
                    if self._is_recent(article) and self._is_tech_related(article):
                        articles.append(article)
            except Exception as e:
                logger.error(f"RSS collection failed: {feed_config['url']}", error=str(e))
        
        return articles
```

**GitHub Collector:**
```python
class GitHubCollector(BaseCollector):
    def collect(self) -> List[Article]:
        articles = []
        
        # GitHub Trending repositories
        trending_repos = self._get_trending_repositories()
        for repo in trending_repos:
            article = Article(
                title=f"üìà Trending: {repo['full_name']}",
                url=repo['html_url'],
                summary=repo['description'],
                published_at=datetime.fromisoformat(repo['created_at']),
                source="github_trending",
                tags=repo['topics'] + ["github", "trending"],
                score=self._calculate_github_score(repo)
            )
            articles.append(article)
        
        # Popular releases
        recent_releases = self._get_popular_releases()
        for release in recent_releases:
            article = self._create_release_article(release)
            articles.append(article)
        
        return articles
    
    def _calculate_github_score(self, repo: dict) -> float:
        """GitHub „É™„Éù„Ç∏„Éà„É™„ÅÆ„Çπ„Ç≥„Ç¢Ë®àÁÆó"""
        stars = repo.get('stargazers_count', 0)
        forks = repo.get('forks_count', 0)
        language = repo.get('language', '')
        
        # Âü∫Êú¨„Çπ„Ç≥„Ç¢ (starÊï∞„Éô„Éº„Çπ)
        base_score = min(stars / 10000, 0.8)
        
        # Ë®ÄË™û„Éú„Éº„Éä„Çπ
        tech_languages = ['Python', 'JavaScript', 'TypeScript', 'Go', 'Rust']
        language_bonus = 0.1 if language in tech_languages else 0
        
        # „Éï„Ç©„Éº„ÇØÁéá„Éú„Éº„Éä„Çπ
        fork_ratio = forks / max(stars, 1)
        fork_bonus = min(fork_ratio * 0.1, 0.1)
        
        return min(base_score + language_bonus + fork_bonus, 1.0)
```

**Reddit Collector:**
```python
class RedditCollector(BaseCollector):
    SUBREDDITS = [
        {"name": "programming", "weight": 0.8},
        {"name": "webdev", "weight": 0.7},
        {"name": "MachineLearning", "weight": 0.9},
        {"name": "technology", "weight": 0.6}
    ]
    
    def collect(self) -> List[Article]:
        reddit = praw.Reddit(
            client_id=self.config.reddit_client_id,
            client_secret=self.config.reddit_client_secret,
            user_agent="TechNewsBot/1.0"
        )
        
        articles = []
        for sub_config in self.SUBREDDITS:
            subreddit = reddit.subreddit(sub_config["name"])
            
            # ‰∫∫Ê∞óÊäïÁ®ø„ÇíÂèñÂæó (24ÊôÇÈñì‰ª•ÂÜÖ)
            for submission in subreddit.hot(limit=20):
                if self._is_recent_submission(submission):
                    article = self._create_reddit_article(submission, sub_config)
                    articles.append(article)
        
        return articles
    
    def _calculate_reddit_score(self, submission, weight: float) -> float:
        """Reddit ÊäïÁ®ø„ÅÆ„Çπ„Ç≥„Ç¢Ë®àÁÆó"""
        upvotes = submission.score
        comments = submission.num_comments
        ratio = submission.upvote_ratio
        
        # Âü∫Êú¨„Çπ„Ç≥„Ç¢
        base_score = min(upvotes / 1000, 0.7)
        
        # „Ç≥„É°„É≥„ÉàÊï∞„Éú„Éº„Éä„Çπ
        comment_bonus = min(comments / 100 * 0.1, 0.2)
        
        # „Ç¢„ÉÉ„Éó„É¥„Ç©„Éº„ÉàÁéá„Éú„Éº„Éä„Çπ
        ratio_bonus = (ratio - 0.5) * 0.2
        
        return min((base_score + comment_bonus + ratio_bonus) * weight, 1.0)
```

### 2. „Éá„Éº„ÇøÂá¶ÁêÜÂ±§ (Processing Layer)

#### 2.1 Content Filter

```python
class ContentFilter:
    """„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞"""
    
    TECH_KEYWORDS = [
        'python', 'javascript', 'react', 'ai', 'machine learning',
        'kubernetes', 'docker', 'aws', 'api', 'framework',
        'open source', 'github', 'programming', 'development'
    ]
    
    SPAM_PATTERNS = [
        r'win \$\d+',
        r'click here',
        r'limited time offer',
        r'buy now'
    ]
    
    def filter(self, articles: List[Article]) -> List[Article]:
        """Ë®ò‰∫ã„Çí„Éï„Ç£„É´„Çø„É™„É≥„Ç∞"""
        filtered = []
        
        for article in articles:
            if self._is_tech_related(article) and not self._is_spam(article):
                filtered.append(article)
        
        return filtered
    
    def _is_tech_related(self, article: Article) -> bool:
        """„ÉÜ„ÉÉ„ÇØÈñ¢ÈÄ£Ë®ò‰∫ã„Åã„Å©„ÅÜ„ÅãÂà§ÂÆö"""
        content = f"{article.title} {article.summary}".lower()
        
        # „Ç≠„Éº„ÉØ„Éº„Éâ„Éû„ÉÉ„ÉÅ„É≥„Ç∞
        keyword_score = sum(1 for keyword in self.TECH_KEYWORDS if keyword in content)
        
        # „Çø„Ç∞„Éô„Éº„Çπ„ÅÆÂà§ÂÆö
        tech_tags = {'programming', 'tech', 'ai', 'ml', 'web', 'mobile'}
        tag_score = len(set(article.tags) & tech_tags)
        
        return keyword_score >= 1 or tag_score >= 1
    
    def _is_spam(self, article: Article) -> bool:
        """„Çπ„Éë„É†Ë®ò‰∫ã„Åã„Å©„ÅÜ„ÅãÂà§ÂÆö"""
        content = f"{article.title} {article.summary}".lower()
        
        for pattern in self.SPAM_PATTERNS:
            if re.search(pattern, content, re.IGNORECASE):
                return True
        
        return False
```

#### 2.2 Deduplicator

```python
class Deduplicator:
    """ÈáçË§áÈô§ÂéªÂá¶ÁêÜ"""
    
    def remove_duplicates(self, articles: List[Article]) -> List[Article]:
        """Ë®ò‰∫ã„ÅÆÈáçË§á„ÇíÈô§Âéª"""
        seen_hashes = set()
        unique_articles = []
        
        # Êó¢Â≠òË®ò‰∫ã„ÅÆ„Éè„ÉÉ„Ç∑„É•„ÇíÂèñÂæó
        existing_hashes = self._get_existing_hashes()
        seen_hashes.update(existing_hashes)
        
        for article in articles:
            content_hash = self._generate_content_hash(article)
            article.content_hash = content_hash
            
            if content_hash not in seen_hashes:
                # URLÈ°û‰ººÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
                if not self._is_similar_url_exists(article, unique_articles):
                    unique_articles.append(article)
                    seen_hashes.add(content_hash)
        
        return unique_articles
    
    def _generate_content_hash(self, article: Article) -> str:
        """„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Éè„ÉÉ„Ç∑„É•ÁîüÊàê"""
        content = f"{article.title}{article.url}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]
    
    def _is_similar_url_exists(self, article: Article, existing: List[Article]) -> bool:
        """È°û‰ººURL„ÅÆÂ≠òÂú®„ÉÅ„Çß„ÉÉ„ÇØ"""
        from difflib import SequenceMatcher
        
        for existing_article in existing:
            similarity = SequenceMatcher(None, article.url, existing_article.url).ratio()
            if similarity > 0.8:  # 80%‰ª•‰∏ä„ÅÆÈ°û‰ººÂ∫¶
                return True
        
        return False
```

#### 2.3 AI Processor

```python
class AIProcessor:
    """AI „Çí‰ΩøÁî®„Åó„ÅüË®ò‰∫ãÂá¶ÁêÜ"""
    
    def __init__(self):
        self.openai_client = OpenAI(api_key=config.openai_api_key)
    
    def process(self, articles: List[Article]) -> List[Article]:
        """Ë®ò‰∫ã„ÇíAIÂá¶ÁêÜ"""
        processed_articles = []
        
        for article in articles:
            try:
                # Ë¶ÅÁ¥ÑÁîüÊàêÔºàË¶ÅÁ¥Ñ„Åå„Å™„ÅÑÂ†¥Âêà„ÅÆ„ÅøÔºâ
                if not article.summary or len(article.summary) < 50:
                    article.summary = self._generate_summary(article)
                
                # „Çπ„Ç≥„Ç¢Ë®àÁÆó
                article.score = self._calculate_ai_score(article)
                
                # „Çø„Ç∞Êã°ÂÖÖ
                article.tags.extend(self._extract_ai_tags(article))
                
                processed_articles.append(article)
                
            except Exception as e:
                logger.error(f"AI processing failed for article: {article.title}", error=str(e))
                # AIÂá¶ÁêÜÂ§±ÊïóÊôÇ„ÅØ„Åù„ÅÆ„Åæ„ÅæËøΩÂä†
                processed_articles.append(article)
        
        return processed_articles
    
    def _generate_summary(self, article: Article) -> str:
        """Ë®ò‰∫ãË¶ÅÁ¥Ñ„ÇíÁîüÊàê"""
        prompt = f"""
        ‰ª•‰∏ã„ÅÆÊäÄË°ìË®ò‰∫ã„ÅÆ„Çø„Ç§„Éà„É´„Å®URL„Åã„Çâ„ÄÅÊó•Êú¨Ë™û„ÅßÁ∞°ÊΩî„Å™Ë¶ÅÁ¥ÑÔºà50-100ÊñáÂ≠óÔºâ„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        ÊäÄË°ìÁöÑ„Å™ÂÜÖÂÆπ„ÇíÈáçË¶ñ„Åó„ÄÅË™≠ËÄÖ„Å´„Å®„Å£„Å¶ÊúâÁõä„Å™ÊÉÖÂ†±„ÇíÂê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        
        „Çø„Ç§„Éà„É´: {article.title}
        URL: {article.url}
        """
        
        response = self.openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150,
            temperature=0.3
        )
        
        return response.choices[0].message.content.strip()
    
    def _calculate_ai_score(self, article: Article) -> float:
        """AI „Å´„Çà„ÇãË®ò‰∫ã„Çπ„Ç≥„Ç¢Ë®àÁÆó"""
        prompt = f"""
        ‰ª•‰∏ã„ÅÆÊäÄË°ìË®ò‰∫ã„ÅÆÈáçË¶ÅÂ∫¶„Çí0.0„Äú1.0„ÅÆ„Çπ„Ç≥„Ç¢„ÅßË©ï‰æ°„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        Ë©ï‰æ°Âü∫Ê∫ñ:
        - ÊäÄË°ìÁöÑÊñ∞Ë¶èÊÄß„ÉªÈù©Êñ∞ÊÄß
        - ÂÆüÁî®ÊÄß„ÉªÊúâÁî®ÊÄß
        - ÂΩ±ÈüøÂäõ„ÉªË©±È°åÊÄß
        - ÊÉÖÂ†±„ÅÆ‰ø°È†ºÊÄß
        
        „Çø„Ç§„Éà„É´: {article.title}
        Ë¶ÅÁ¥Ñ: {article.summary}
        „ÇΩ„Éº„Çπ: {article.source}
        
        Êï∞ÂÄ§„ÅÆ„ÅøËøîÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºà‰æã: 0.75Ôºâ
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=10,
                temperature=0.1
            )
            
            score_text = response.choices[0].message.content.strip()
            return float(score_text)
            
        except Exception:
            # AI „Çπ„Ç≥„Ç¢Ë®àÁÆóÂ§±ÊïóÊôÇ„ÅØÂü∫Êú¨„Çπ„Ç≥„Ç¢„Çí‰ΩøÁî®
            return self._calculate_basic_score(article)
    
    def _calculate_basic_score(self, article: Article) -> float:
        """Âü∫Êú¨ÁöÑ„Å™„Çπ„Ç≥„Ç¢Ë®àÁÆóÔºàAIÂ§±ÊïóÊôÇ„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ"""
        score = 0.5  # „Éô„Éº„Çπ„Çπ„Ç≥„Ç¢
        
        # „ÇΩ„Éº„ÇπÂà•Èáç„Åø‰ªò„Åë
        source_weights = {
            'hackernews': 0.9,
            'github_trending': 0.8,
            'techcrunch': 0.7,
            'zenn': 0.6,
            'reddit': 0.5
        }
        
        source_weight = source_weights.get(article.source, 0.5)
        score *= source_weight
        
        # „Çø„Ç§„Éà„É´Èï∑„Éú„Éº„Éä„ÇπÔºàÈÅ©Â∫¶„Å™Èï∑„ÅïÔºâ
        title_len = len(article.title)
        if 20 <= title_len <= 100:
            score += 0.1
        
        return min(score, 1.0)
```

### 3. „Éá„Éº„Çø‰øùÂ≠òÂ±§ (Storage Layer)

#### 3.1 DynamoDB „ÉÜ„Éº„Éñ„É´Ë®≠Ë®à

```yaml
Table: tech-news-articles
Properties:
  BillingMode: PAY_PER_REQUEST
  
Attributes:
  date: String (YYYY-MM-DD) # Partition Key
  article_id: String (UUID) # Sort Key
  title: String
  url: String
  summary: String
  published_at: Number (Unix timestamp)
  source: String
  tags: List
  score: Number (0.0-1.0)
  content_hash: String
  author: String (Optional)
  ttl: Number (Ëá™ÂãïÂâäÈô§Áî®)

Global Secondary Indexes:
  1. source-published_at-index
     - PK: source
     - SK: published_at
     - Purpose: „ÇΩ„Éº„ÇπÂà•„ÅÆË®ò‰∫ãÂèñÂæó
  
  2. content-hash-index
     - PK: content_hash
     - Purpose: ÈáçË§áÊ§úÂá∫
  
  3. score-index
     - PK: date
     - SK: score
     - Purpose: È´ò„Çπ„Ç≥„Ç¢Ë®ò‰∫ã„ÅÆÂèñÂæó
```

#### 3.2 Article Store Implementation

```python
class ArticleStore:
    """DynamoDB „Çí‰ΩøÁî®„Åó„ÅüË®ò‰∫ã„Çπ„Éà„É¨„Éº„Ç∏"""
    
    def __init__(self):
        self.dynamodb = boto3.resource('dynamodb')
        self.table = self.dynamodb.Table(config.dynamodb_table)
    
    def bulk_save(self, articles: List[Article]) -> int:
        """Ë®ò‰∫ã„Çí‰∏ÄÊã¨‰øùÂ≠ò"""
        saved_count = 0
        
        with self.table.batch_writer() as batch:
            for article in articles:
                try:
                    item = self._article_to_dynamodb_item(article)
                    batch.put_item(Item=item)
                    saved_count += 1
                except Exception as e:
                    logger.error(f"Failed to save article: {article.title}", error=str(e))
        
        return saved_count
    
    def get_articles_by_date(self, date: datetime.date, min_score: float = 0.0) -> List[Article]:
        """Êó•‰ªòÊåáÂÆö„ÅßË®ò‰∫ã„ÇíÂèñÂæó"""
        date_str = date.strftime('%Y-%m-%d')
        
        response = self.table.query(
            KeyConditionExpression=Key('date').eq(date_str),
            FilterExpression=Attr('score').gte(min_score),
            ScanIndexForward=False  # Êñ∞„Åó„ÅÑÈ†Ü
        )
        
        articles = []
        for item in response['Items']:
            article = self._dynamodb_item_to_article(item)
            articles.append(article)
        
        return articles
    
    def get_existing_hashes(self, days: int = 7) -> Set[str]:
        """ÈÅéÂéªNÊó•Èñì„ÅÆË®ò‰∫ã„Éè„ÉÉ„Ç∑„É•„ÇíÂèñÂæó"""
        hashes = set()
        
        for i in range(days):
            date = datetime.date.today() - datetime.timedelta(days=i)
            date_str = date.strftime('%Y-%m-%d')
            
            response = self.table.query(
                KeyConditionExpression=Key('date').eq(date_str),
                ProjectionExpression='content_hash'
            )
            
            for item in response['Items']:
                hashes.add(item['content_hash'])
        
        return hashes
    
    def _article_to_dynamodb_item(self, article: Article) -> dict:
        """Article „Çí DynamoDB „Ç¢„Ç§„ÉÜ„É†„Å´Â§âÊèõ"""
        date_str = article.published_at.strftime('%Y-%m-%d')
        ttl = int((datetime.now() + timedelta(days=30)).timestamp())
        
        return {
            'date': date_str,
            'article_id': str(uuid.uuid4()),
            'title': article.title,
            'url': article.url,
            'summary': article.summary,
            'published_at': int(article.published_at.timestamp()),
            'source': article.source,
            'tags': article.tags,
            'score': Decimal(str(article.score)),
            'content_hash': article.content_hash,
            'author': article.author,
            'ttl': ttl
        }
```

### 4. ÈÄöÁü•Â±§ (Notification Layer)

#### 4.1 Notifier Lambda Function

```python
def notifier_handler(event, context):
    """SlackÈÄöÁü•„ÅÆ„É°„Ç§„É≥„Éè„É≥„Éâ„É©„Éº"""
    
    # Êú¨Êó•„ÅÆÈ´ò„Çπ„Ç≥„Ç¢Ë®ò‰∫ã„ÇíÂèñÂæó
    article_store = ArticleStore()
    today_articles = article_store.get_articles_by_date(
        datetime.date.today(),
        min_score=0.6  # „Çπ„Ç≥„Ç¢0.6‰ª•‰∏ä
    )
    
    if not today_articles:
        logger.info("No articles to notify")
        return {"statusCode": 200, "message": "No articles"}
    
    # „Çπ„Ç≥„Ç¢È†Ü„Åß„ÇΩ„Éº„Éà
    sorted_articles = sorted(today_articles, key=lambda x: x.score, reverse=True)
    top_articles = sorted_articles[:10]  # ‰∏ä‰Ωç10Ë®ò‰∫ã
    
    # ÊôÇÈñìÂ∏Ø„Å´„Çà„ÇãÂàÜÈ°û
    current_hour = datetime.now().hour
    if current_hour < 12:
        notification_type = "morning"
        title = "üåÖ ‰ªäÊó•„ÅÆ„ÉÜ„ÉÉ„ÇØ„Éã„É•„Éº„ÇπÔºàÊúùÂàäÔºâ"
    else:
        notification_type = "evening"
        title = "üåÜ ‰ªäÊó•„ÅÆ„ÉÜ„ÉÉ„ÇØ„Éã„É•„Éº„ÇπÔºàÂ§ïÂàäÔºâ"
    
    # SlackÈÄöÁü•
    slack_notifier = SlackNotifier()
    success = slack_notifier.send_daily_summary(top_articles, title, notification_type)
    
    # „É°„Éà„É™„ÇØ„ÇπÈÄÅ‰ø°
    cloudwatch = boto3.client('cloudwatch')
    cloudwatch.put_metric_data(
        Namespace='TechNewsBot',
        MetricData=[
            {
                'MetricName': 'NotificationsSent',
                'Value': 1 if success else 0,
                'Unit': 'Count',
                'Dimensions': [
                    {'Name': 'Type', 'Value': notification_type}
                ]
            },
            {
                'MetricName': 'ArticlesNotified',
                'Value': len(top_articles),
                'Unit': 'Count'
            }
        ]
    )
    
    return {
        "statusCode": 200 if success else 500,
        "articles_sent": len(top_articles),
        "notification_type": notification_type
    }
```

#### 4.2 Slack Notifier Implementation

```python
class SlackNotifier:
    """Slack ÈÄöÁü•Âá¶ÁêÜ"""
    
    def __init__(self):
        self.webhook_url = config.slack_webhook_url
    
    def send_daily_summary(self, articles: List[Article], title: str, notification_type: str) -> bool:
        """Êó•Ê¨°„Çµ„Éû„É™„Éº„ÇíSlack„Å´ÈÄÅ‰ø°"""
        
        # „Éò„ÉÉ„ÉÄ„Éº„Éñ„É≠„ÉÉ„ÇØ
        blocks = [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": title
                }
            },
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": f"üìä {len(articles)}‰ª∂„ÅÆË®ò‰∫ã„Çí„ÅäÂ±ä„Åë„Åó„Åæ„Åô"
                    }
                ]
            },
            {"type": "divider"}
        ]
        
        # Ë®ò‰∫ã„Éñ„É≠„ÉÉ„ÇØ
        for i, article in enumerate(articles, 1):
            blocks.extend(self._create_article_blocks(article, i))
            
            # 5Ë®ò‰∫ã„Åî„Å®„Å´Âå∫Âàá„ÇäÁ∑ö
            if i % 5 == 0 and i < len(articles):
                blocks.append({"type": "divider"})
        
        # „Éï„ÉÉ„Çø„Éº„Éñ„É≠„ÉÉ„ÇØ
        blocks.extend([
            {"type": "divider"},
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": f"ü§ñ Tech News Bot | {datetime.now().strftime('%Y-%m-%d %H:%M')}"
                    }
                ]
            }
        ])
        
        payload = {
            "blocks": blocks,
            "username": "Tech News Bot",
            "icon_emoji": ":newspaper:"
        }
        
        try:
            response = requests.post(
                self.webhook_url,
                json=payload,
                timeout=10
            )
            
            if response.status_code == 200:
                logger.info(f"Successfully sent {len(articles)} articles to Slack")
                return True
            else:
                logger.error(f"Slack notification failed: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"Slack notification error: {str(e)}")
            return False
    
    def _create_article_blocks(self, article: Article, index: int) -> List[dict]:
        """ÂÄãÂà•Ë®ò‰∫ã„ÅÆSlack„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê"""
        
        # „Çπ„Ç≥„Ç¢„Å´Âøú„Åò„ÅüÁµµÊñáÂ≠ó
        if article.score >= 0.9:
            score_emoji = "üî•"
        elif article.score >= 0.8:
            score_emoji = "‚≠ê"
        elif article.score >= 0.7:
            score_emoji = "üëç"
        else:
            score_emoji = "üìÑ"
        
        # „ÇΩ„Éº„ÇπÁµµÊñáÂ≠ó„Éû„ÉÉ„Éî„É≥„Ç∞
        source_emojis = {
            'hackernews': 'üü†',
            'github_trending': 'üêô',
            'techcrunch': 'üü¢',
            'zenn': 'üáØüáµ',
            'reddit': 'üî∂'
        }
        source_emoji = source_emojis.get(article.source, 'üì∞')
        
        # „Çø„Ç∞„ÅÆË°®Á§∫ÔºàÊúÄÂ§ß3ÂÄãÔºâ
        tag_text = ""
        if article.tags:
            displayed_tags = article.tags[:3]
            tag_text = " | " + " ".join([f"`{tag}`" for tag in displayed_tags])
        
        return [
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"{score_emoji} *{index}. {article.title}*\n{article.summary}"
                },
                "accessory": {
                    "type": "button",
                    "text": {
                        "type": "plain_text",
                        "text": "Read More",
                        "emoji": True
                    },
                    "url": article.url,
                    "action_id": f"read_article_{index}"
                }
            },
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": f"{source_emoji} {article.source} | Score: {article.score:.2f}{tag_text}"
                    }
                ]
            }
        ]
```

### 5. Áõ£Ë¶ñ„ÉªÈÅãÁî®Â±§ (Monitoring Layer)

#### 5.1 CloudWatch „Ç´„Çπ„Çø„É†„É°„Éà„É™„ÇØ„Çπ

```python
class MetricsCollector:
    """„Ç´„Çπ„Çø„É†„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ"""
    
    def __init__(self):
        self.cloudwatch = boto3.client('cloudwatch')
    
    def send_collection_metrics(self, source: str, articles_count: int, processing_time: float, errors: int = 0):
        """ÂèéÈõÜ„É°„Éà„É™„ÇØ„Çπ„ÇíÈÄÅ‰ø°"""
        self.cloudwatch.put_metric_data(
            Namespace='TechNewsBot/Collection',
            MetricData=[
                {
                    'MetricName': 'ArticlesCollected',
                    'Dimensions': [{'Name': 'Source', 'Value': source}],
                    'Value': articles_count,
                    'Unit': 'Count',
                    'Timestamp': datetime.utcnow()
                },
                {
                    'MetricName': 'ProcessingTime',
                    'Dimensions': [{'Name': 'Source', 'Value': source}],
                    'Value': processing_time,
                    'Unit': 'Seconds'
                },
                {
                    'MetricName': 'CollectionErrors',
                    'Dimensions': [{'Name': 'Source', 'Value': source}],
                    'Value': errors,
                    'Unit': 'Count'
                }
            ]
        )
    
    def send_processing_metrics(self, duplicates_removed: int, ai_failures: int, total_processed: int):
        """Âá¶ÁêÜ„É°„Éà„É™„ÇØ„Çπ„ÇíÈÄÅ‰ø°"""
        self.cloudwatch.put_metric_data(
            Namespace='TechNewsBot/Processing',
            MetricData=[
                {
                    'MetricName': 'DuplicatesRemoved',
                    'Value': duplicates_removed,
                    'Unit': 'Count'
                },
                {
                    'MetricName': 'AIProcessingFailures',
                    'Value': ai_failures,
                    'Unit': 'Count'
                },
                {
                    'MetricName': 'ArticlesProcessed',
                    'Value': total_processed,
                    'Unit': 'Count'
                }
            ]
        )
```

#### 5.2 CloudWatch „Ç¢„É©„Éº„É†Ë®≠ÂÆö

```yaml
# CloudFormation Template for Alarms
Resources:
  # ÂèéÈõÜ„Ç®„É©„Éº„Ç¢„É©„Éº„É†
  CollectionErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: TechNewsBot-CollectionErrors
      AlarmDescription: "High error rate in article collection"
      MetricName: Errors
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref CollectorFunction
      AlarmActions:
        - !Ref SNSAlertTopic
  
  # Ë®ò‰∫ãÂèéÈõÜÊï∞„Ç¢„É©„Éº„É†
  NoArticlesAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: TechNewsBot-NoArticlesCollected
      AlarmDescription: "No articles collected in recent execution"
      MetricName: ArticlesCollected
      Namespace: TechNewsBot/Collection
      Statistic: Sum
      Period: 3600
      EvaluationPeriods: 1
      Threshold: 5
      ComparisonOperator: LessThanThreshold
      TreatMissingData: breaching
      AlarmActions:
        - !Ref SNSAlertTopic
  
  # LambdaÂÆüË°åÊôÇÈñì„Ç¢„É©„Éº„É†
  LongExecutionAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: TechNewsBot-LongExecution
      AlarmDescription: "Lambda execution time too long"
      MetricName: Duration
      Namespace: AWS/Lambda
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: 240000  # 4ÂàÜ
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref CollectorFunction
```

#### 5.3 ÊßãÈÄ†Âåñ„É≠„Ç∞ÂÆüË£Ö

```python
import structlog
import json
from datetime import datetime

# „É≠„Ç∞Ë®≠ÂÆö
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.dev.ConsoleRenderer() if DEBUG else structlog.processors.JSONRenderer()
    ],
    wrapper_class=structlog.stdlib.LoggerFactory(),
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# ‰ΩøÁî®‰æã
class BaseCollector:
    def __init__(self):
        self.logger = structlog.get_logger(self.__class__.__name__)
    
    def collect(self) -> List[Article]:
        start_time = time.time()
        
        try:
            articles = self._do_collect()
            processing_time = time.time() - start_time
            
            self.logger.info(
                "collection_completed",
                source=self.source_name,
                articles_collected=len(articles),
                processing_time_seconds=processing_time,
                success=True
            )
            
            return articles
            
        except Exception as e:
            processing_time = time.time() - start_time
            
            self.logger.error(
                "collection_failed",
                source=self.source_name,
                error=str(e),
                error_type=type(e).__name__,
                processing_time_seconds=processing_time,
                success=False
            )
            
            raise
```

## „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ

### 1. ‰∏¶Ë°åÂá¶ÁêÜ„ÅÆÂÆüË£Ö

```python
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor, as_completed

class AsyncCollector:
    """ÈùûÂêåÊúü„Éá„Éº„ÇøÂèéÈõÜ"""
    
    async def collect_all_sources(self) -> List[Article]:
        """ÂÖ®„ÇΩ„Éº„Çπ„Åã„Çâ‰∏¶Ë°åÂèéÈõÜ"""
        
        tasks = [
            self.collect_rss_async(),
            self.collect_github_async(),
            self.collect_reddit_async()
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_articles = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Collection failed: {result}")
            else:
                all_articles.extend(result)
        
        return all_articles
    
    async def collect_rss_async(self) -> List[Article]:
        """RSSÈùûÂêåÊúüÂèéÈõÜ"""
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            for feed_config in RSS_FEEDS:
                task = self.fetch_rss_feed(session, feed_config)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            articles = []
            for result in results:
                if not isinstance(result, Exception):
                    articles.extend(result)
            
            return articles
```

### 2. „Ç≠„É£„ÉÉ„Ç∑„É•Êà¶Áï•

```python
import redis
from functools import wraps

class CacheManager:
    """Redis „Ç≠„É£„ÉÉ„Ç∑„É•„Éû„Éç„Éº„Ç∏„É£„Éº"""
    
    def __init__(self):
        self.redis_client = redis.Redis(
            host=config.redis_host,
            port=config.redis_port,
            decode_responses=True
        )
    
    def cache_articles(self, key: str, articles: List[Article], ttl: int = 3600):
        """Ë®ò‰∫ã„Çí„Ç≠„É£„ÉÉ„Ç∑„É•"""
        serialized = json.dumps([article.__dict__ for article in articles], default=str)
        self.redis_client.setex(key, ttl, serialized)
    
    def get_cached_articles(self, key: str) -> Optional[List[Article]]:
        """„Ç≠„É£„ÉÉ„Ç∑„É•„Åã„ÇâË®ò‰∫ã„ÇíÂèñÂæó"""
        cached = self.redis_client.get(key)
        if cached:
            data = json.loads(cached)
            return [Article(**item) for item in data]
        return None

def cache_result(cache_key: str, ttl: int = 3600):
    """„Éá„Ç≥„É¨„Éº„ÇøÔºöÁµêÊûú„Çí„Ç≠„É£„ÉÉ„Ç∑„É•"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            cache_manager = CacheManager()
            
            # „Ç≠„É£„ÉÉ„Ç∑„É•„ÉÅ„Çß„ÉÉ„ÇØ
            cached_result = cache_manager.get_cached_articles(cache_key)
            if cached_result:
                return cached_result
            
            # ÂÆüË°å & „Ç≠„É£„ÉÉ„Ç∑„É•
            result = func(*args, **kwargs)
            cache_manager.cache_articles(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

### 3. „Éê„ÉÉ„ÉÅÂá¶ÁêÜÊúÄÈÅ©Âåñ

```python
class BatchProcessor:
    """„Éê„ÉÉ„ÉÅÂá¶ÁêÜÊúÄÈÅ©Âåñ"""
    
    def __init__(self, batch_size: int = 25):
        self.batch_size = batch_size
    
    def process_articles_in_batches(self, articles: List[Article]) -> List[Article]:
        """Ë®ò‰∫ã„Çí„Éê„ÉÉ„ÉÅÂá¶ÁêÜ"""
        processed_articles = []
        
        for i in range(0, len(articles), self.batch_size):
            batch = articles[i:i + self.batch_size]
            batch_result = self._process_batch(batch)
            processed_articles.extend(batch_result)
            
            # „É¨„Éº„ÉàÂà∂ÈôêÂØæÂøú
            time.sleep(0.1)
        
        return processed_articles
    
    def _process_batch(self, batch: List[Article]) -> List[Article]:
        """„Éê„ÉÉ„ÉÅÂçò‰Ωç„ÅÆÂá¶ÁêÜ"""
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            
            for article in batch:
                future = executor.submit(self._process_single_article, article)
                futures.append(future)
            
            results = []
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=30)
                    results.append(result)
                except Exception as e:
                    logger.error(f"Batch processing failed: {e}")
            
            return results
```

„Åì„ÅÆË©≥Á¥∞Ë®≠Ë®à„Å´„Çà„Çä„ÄÅ„Çπ„Ç±„Éº„É©„Éñ„É´„Åß‰øùÂÆàÊÄß„ÅÆÈ´ò„ÅÑ„ÉÜ„ÉÉ„ÇØ„Éã„É•„Éº„ÇπÂèéÈõÜ„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åß„Åç„Åæ„Åô„ÄÇÂêÑ„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÅØÁã¨Á´ãÊÄß„Çí‰øù„Å°„Å™„Åå„Çâ„ÄÅÂäπÁéáÁöÑ„Å´ÈÄ£Êê∫„Åô„ÇãË®≠Ë®à„Å®„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ